This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
dags/
  rural_fitbit_processing_dag.py
glue_scripts/
  config.py
  load_staging.py
  merge_production.py
  merge_queries.py
  metadata_handler.py
  process_activity.py
  schemas.py
.gitignore
README.md

================================================================
Files
================================================================

================
File: dags/rural_fitbit_processing_dag.py
================
from airflow import DAG
from airflow.providers.amazon.aws.operators.glue import GlueJobOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta
from airflow.models import Variable
from glue_scripts.config import (
    S3_CONFIG, AWS_CONFIG, get_glue_job_params, get_redshift_params
)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

ACTIVITY_TYPES = ['heart', 'calories', 'steps', 'elevation', 'distance', 'floors']

dag = DAG(
    'rural_fitbit_processing',
    default_args=default_args,
    description='Process Rural Fitbit data from S3',
    schedule_interval='0 0 * * 0',  # Weekly on Sunday at midnight
    catchup=False
)

# Sensor to check for new zip files
check_s3_files = S3KeySensor(
    task_id='check_s3_files',
    bucket_key='Onboarding/RADx/RURAL/fitbit_intraday/*.zip',
    bucket_name='umms-research-radx-image-processing',
    aws_conn_id='aws_default',
    timeout=60 * 60,  # 1 hour timeout
    poke_interval=60,  # Check every minute
    dag=dag
)

# Create parallel activity processing tasks
with TaskGroup(group_id='process_activities', dag=dag) as activity_group:
    for activity in ACTIVITY_TYPES:
        GlueJobOperator(
            task_id=f'process_{activity}',
            job_name=f"{AWS_CONFIG['GLUE_JOB_PREFIX']}_{activity}_processor",
            script_location=f"s3://{S3_CONFIG['GLUE_SCRIPTS_BUCKET']}/process_activity.py",
            job_parameters=get_glue_job_params(activity),
            aws_conn_id='aws_default',
            region_name=AWS_CONFIG['REGION'],
            dag=dag
        )

# Load to staging tables
load_staging = GlueJobOperator(
    task_id='load_staging_tables',
    job_name=f"{AWS_CONFIG['GLUE_JOB_PREFIX']}_staging_loader",
    script_location=f"s3://{S3_CONFIG['GLUE_SCRIPTS_BUCKET']}/load_staging.py",
    job_parameters=get_redshift_params(),
    aws_conn_id='aws_default',
    region_name=AWS_CONFIG['REGION'],
    dag=dag
)

# Merge to production
merge_production = GlueJobOperator(
    task_id='merge_production_tables',
    job_name='rural_fitbit_production_merger',
    script_location='s3://your-glue-scripts-bucket/merge_production.py',
    job_parameters={
        '--REDSHIFT_DATABASE': '{{ conn.redshift_default.schema }}',
        '--REDSHIFT_USER': '{{ conn.redshift_default.login }}',
        '--REDSHIFT_PASSWORD': '{{ conn.redshift_default.password }}',
        '--REDSHIFT_HOST': '{{ conn.redshift_default.host }}',
        '--REDSHIFT_PORT': '{{ conn.redshift_default.port }}'
    },
    aws_conn_id='aws_default',
    region_name='your-region',
    dag=dag
)

# Set up dependencies
check_s3_files >> activity_group >> load_staging >> merge_production

================
File: glue_scripts/config.py
================
import os
from typing import Dict, Any

# S3 Configuration
S3_CONFIG = {
    'SOURCE_BUCKET': os.getenv('FITBIT_SOURCE_BUCKET', 'umms-research-radx-image-processing'),
    'SOURCE_PREFIX': os.getenv('FITBIT_SOURCE_PREFIX', 'Onboarding/RADx/RURAL/fitbit_intraday'),
    'DEST_BUCKET': os.getenv('FITBIT_DEST_BUCKET', 'umasschan-plum-data-lab'),
    'DEST_PREFIX': os.getenv('FITBIT_DEST_PREFIX', 'Rural/intraday'),
    'GLUE_SCRIPTS_BUCKET': os.getenv('GLUE_SCRIPTS_BUCKET', 'your-glue-scripts-bucket'),
}

# Redshift Configuration
REDSHIFT_CONFIG = {
    'TEMP_DIR': os.getenv('REDSHIFT_TEMP_DIR', 's3://your-temp-bucket/temp/'),
    'DATABASE': os.getenv('REDSHIFT_DATABASE', 'your_database'),
    'SCHEMA': os.getenv('REDSHIFT_SCHEMA', 'rural'),
}

# AWS Configuration
AWS_CONFIG = {
    'REGION': os.getenv('AWS_REGION', 'us-east-1'),
    'GLUE_JOB_PREFIX': os.getenv('GLUE_JOB_PREFIX', 'rural_fitbit'),
}

def get_glue_job_params(activity_type: str = None) -> Dict[str, Any]:
    """Get parameters for Glue job configuration"""
    params = {
        '--source_bucket': S3_CONFIG['SOURCE_BUCKET'],
        '--source_prefix': S3_CONFIG['SOURCE_PREFIX'],
        '--dest_bucket': S3_CONFIG['DEST_BUCKET'],
        '--dest_prefix': S3_CONFIG['DEST_PREFIX'],
    }
    
    if activity_type:
        params['--activity_type'] = activity_type
        
    return params

def get_redshift_params() -> Dict[str, str]:
    """Get Redshift connection parameters from Airflow connection"""
    return {
        '--REDSHIFT_DATABASE': '{{ conn.redshift_default.schema }}',
        '--REDSHIFT_USER': '{{ conn.redshift_default.login }}',
        '--REDSHIFT_PASSWORD': '{{ conn.redshift_default.password }}',
        '--REDSHIFT_HOST': '{{ conn.redshift_default.host }}',
        '--REDSHIFT_PORT': '{{ conn.redshift_default.port }}',
        '--REDSHIFT_SCHEMA': REDSHIFT_CONFIG['SCHEMA'],
        '--REDSHIFT_TEMP_DIR': REDSHIFT_CONFIG['TEMP_DIR'],
    }

================
File: glue_scripts/load_staging.py
================
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

def load_to_staging(glueContext, source_path, table_name):
    # Read parquet files
    dynamic_frame = glueContext.create_dynamic_frame.from_options(
        connection_type="s3",
        connection_options={"paths": [source_path]},
        format="parquet"
    )
    
    # Load to Redshift staging
    glueContext.write_dynamic_frame.from_jdbc_conf(
        frame=dynamic_frame,
        catalog_connection="redshift_connection",
        connection_options={
            "dbtable": f"rural.stg_{table_name}",
            "database": "your_database"
        },
        redshift_tmp_dir="s3://your-temp-bucket/temp/"
    )

def main():
    args = getResolvedOptions(sys.argv, ['JOB_NAME'])
    
    sc = SparkContext()
    glueContext = GlueContext(sc)
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    # Load each activity type to staging
    activities = ['calories', 'distance', 'elevation', 'floors', 'heart', 'steps']
    for activity in activities:
        load_to_staging(
            glueContext,
            f"s3://your-bucket/processed/{activity}",
            f"rural_intraday_{activity}"
        )
    
    job.commit()

if __name__ == "__main__":
    main()

================
File: glue_scripts/merge_production.py
================
import sys
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import psycopg2
from merge_queries import merge_queries  # Import from separate file

def execute_merges(redshift_conn):
    with redshift_conn.cursor() as cursor:
        for table_name, query in merge_queries.items():
            try:
                cursor.execute(query)
                print(f"Merged {table_name} successfully")
            except Exception as e:
                print(f"Error merging {table_name}: {str(e)}")
                raise

def main():
    args = getResolvedOptions(sys.argv, [
        'JOB_NAME',
        'REDSHIFT_DATABASE',
        'REDSHIFT_USER',
        'REDSHIFT_PASSWORD',
        'REDSHIFT_HOST',
        'REDSHIFT_PORT'
    ])
    
    sc = SparkContext()
    glueContext = GlueContext(sc)
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    # Execute merges
    with psycopg2.connect(
        dbname=args['REDSHIFT_DATABASE'],
        user=args['REDSHIFT_USER'],
        password=args['REDSHIFT_PASSWORD'],
        host=args['REDSHIFT_HOST'],
        port=args['REDSHIFT_PORT']
    ) as conn:
        execute_merges(conn)
        conn.commit()
    
    job.commit()

if __name__ == "__main__":
    main()

================
File: glue_scripts/merge_queries.py
================
from typing import Dict, Any
from config import REDSHIFT_CONFIG

# Define all MERGE queries for each activity type
MERGE_QUERIES = {
    "Calories Daily": """
        MERGE INTO rural.rural_intraday_calories_daily
        USING rural.stg_rural_intraday_calories_daily
        ON rural.rural_intraday_calories_daily.participantidentifier = rural.stg_rural_intraday_calories_daily.participantidentifier
            AND rural.rural_intraday_calories_daily.date = rural.stg_rural_intraday_calories_daily.date
        REMOVE DUPLICATES;
    """,
    
    "Calories": """
        MERGE INTO rural.rural_intraday_calories
        USING rural.stg_rural_intraday_calories
        ON rural.rural_intraday_calories.participantidentifier = rural.stg_rural_intraday_calories.participantidentifier
            AND rural.rural_intraday_calories.datetime = rural.stg_rural_intraday_calories.datetime
        REMOVE DUPLICATES;
    """,
    
    "Heart Zones": """
        MERGE INTO rural.rural_intraday_heart_zones
        USING rural.stg_rural_intraday_heart_zones
        ON rural.rural_intraday_heart_zones.participantidentifier = rural.stg_rural_intraday_heart_zones.participantidentifier
            AND rural.rural_intraday_heart_zones.date = rural.stg_rural_intraday_heart_zones.date
            AND rural.rural_intraday_heart_zones.zone_name = rural.stg_rural_intraday_heart_zones.zone_name
        REMOVE DUPLICATES;
    """,
    
    # ... (similar queries for other activities)
}

def execute_merge(redshift_conn) -> Dict[str, Any]:
    """
    Execute all merge operations and return results
    Args:
        redshift_conn: Redshift connection object
    Returns:
        Dict containing execution results
    """
    results = {
        'successful_merges': [],
        'failed_merges': [],
        'errors': {}
    }
    
    with redshift_conn.cursor() as cursor:
        for query_name, query in MERGE_QUERIES.items():
            try:
                print(f"Executing {query_name} merge...")
                cursor.execute(query)
                results['successful_merges'].append(query_name)
                print(f"{query_name} merge completed successfully")
            except Exception as e:
                error_msg = f"Error in {query_name} merge: {str(e)}"
                print(error_msg)
                results['failed_merges'].append(query_name)
                results['errors'][query_name] = str(e)
        
        if not results['failed_merges']:
            redshift_conn.commit()
            print("All merges completed successfully")
        else:
            redshift_conn.rollback()
            print(f"Rolling back due to {len(results['failed_merges'])} failed merges")
    
    return results

def print_execution_summary(results: Dict[str, Any]) -> None:
    """
    Print a summary of the merge execution results
    Args:
        results: Dictionary containing execution results
    """
    print("\nExecution Summary:")
    print(f"Successful merges: {len(results['successful_merges'])}")
    print(f"Failed merges: {len(results['failed_merges'])}")
    
    if results['errors']:
        print("\nErrors encountered:")
        for query_name, error in results['errors'].items():
            print(f"{query_name}: {error}")

if __name__ == '__main__':
    # This section is for local testing only
    import redshift_conn
    
    try:
        conn = redshift_conn.RedshiftEngine()
        results = execute_merge(conn)
        print_execution_summary(results)
    except Exception as e:
        print(f"Error establishing connection: {str(e)}")
    finally:
        if 'conn' in locals():
            conn.close()

================
File: glue_scripts/metadata_handler.py
================
from datetime import datetime
import boto3
import json

class MetadataHandler:
    def __init__(self, activity_type):
        self.activity_type = activity_type
        self.metadata = {
            'metadata_file': {},
            'processing_stats': {
                'total_files': 0,
                'duplicates': 0,
                'zero_values': 0,
                'processed': 0
            }
        }
    
    def is_processed(self, participant_id: str, date_str: str) -> bool:
        """Check if a specific record has been processed"""
        if not date_str:
            return False
            
        try:
            year, month, day = date_str.split('-')
            return self.metadata.get('metadata_file', {})\
                          .get(year, {})\
                          .get(month, {})\
                          .get(day, {})\
                          .get(self.activity_type, {})\
                          .get(participant_id, False)
        except:
            return False
    
    def record_processed(self, participant_id: str, date_str: str, file_path: str) -> None:
        """Mark a record as processed"""
        if not date_str:
            return
            
        year, month, day = date_str.split('-')
        
        # Create nested structure
        self.metadata.setdefault('metadata_file', {})
        self.metadata['metadata_file'].setdefault(year, {})
        self.metadata['metadata_file'][year].setdefault(month, {})
        self.metadata['metadata_file'][year][month].setdefault(day, {})
        self.metadata['metadata_file'][year][month][day].setdefault(self.activity_type, {})
        
        # Mark as processed
        self.metadata['metadata_file'][year][month][day][self.activity_type][participant_id] = True
    
    def update_stats(self, stat_type: str):
        """Update processing statistics"""
        if stat_type in self.metadata['processing_stats']:
            self.metadata['processing_stats'][stat_type] += 1
    
    def get_stats(self):
        """Get current processing statistics"""
        return self.metadata['processing_stats']
    
    def save_to_s3(self, bucket: str, prefix: str):
        """Save metadata to S3 for debugging/tracking"""
        s3 = boto3.client('s3')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        key = f"{prefix}/metadata/{self.activity_type}_{timestamp}.json"
        
        s3.put_object(
            Bucket=bucket,
            Key=key,
            Body=json.dumps(self.metadata)
        )

================
File: glue_scripts/process_activity.py
================
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import boto3
from datetime import datetime
import json
from schemas import activity_schemas, schema_mapping
from metadata_handler import MetadataHandler
import re
from typing import List, Tuple, Dict, Any

def process_activity_files(glueContext, activity_type, source_bucket, source_prefix):
    metadata_handler = MetadataHandler(activity_type)
    
    # Process files
    for file in list_files(source_bucket, source_prefix):
        participant_id = extract_participant_id(file)
        date_str = extract_date(file)
        
        # Check if already processed
        if metadata_handler.is_processed(participant_id, date_str):
            metadata_handler.update_stats('duplicates')
            continue
            
        # Process file
        try:
            process_file(file, schema_mapping[activity_type])
            metadata_handler.record_processed(participant_id, date_str, file)
            metadata_handler.update_stats('processed')
        except Exception as e:
            print(f"Error processing file: {str(e)}")
    
    # Save metadata to S3 for tracking
    metadata_handler.save_to_s3(source_bucket, source_prefix)
    
    return metadata_handler.get_stats()

def extract_activity_data(data, activity_type, participant_id):
    daily_data = []
    intraday_data = []
    
    # Extract daily summary
    daily_summary = data.get(f'activities-{activity_type}', [])
    for summary in daily_summary:
        if activity_type == 'heart':
            # Special processing for heart rate zones
            process_heart_zones(summary, participant_id, daily_data)
        else:
            # Standard processing for other activities
            process_standard_daily(summary, participant_id, activity_type, daily_data)
    
    # Extract intraday data
    intraday_key = f'activities-{activity_type}-intraday'
    if intraday_key in data:
        process_intraday_data(
            data[intraday_key].get('dataset', []),
            participant_id,
            activity_type,
            intraday_data
        )
    
    return daily_data, intraday_data

def list_files(bucket: str, prefix: str) -> List[str]:
    """
    List all JSON files in the specified S3 bucket and prefix
    """
    s3 = boto3.client('s3')
    files = []
    paginator = s3.get_paginator('list_objects_v2')
    
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].endswith('.json'):
                    files.append(obj['Key'])
    
    return files

def extract_participant_id(file_path: str) -> str:
    """
    Extract participant ID from file path
    Expected format: path/to/PARTICIPANT_ID/date/file.json
    """
    parts = file_path.split('/')
    for part in parts:
        # Assuming participant IDs follow a specific pattern
        # Modify this regex pattern to match your participant ID format
        if re.match(r'^[A-Z0-9]{6,}$', part):
            return part
    raise ValueError(f"Could not extract participant ID from {file_path}")

def extract_date(file_path: str) -> str:
    """
    Extract date from file path
    Expected format: path/to/participant/YYYY-MM-DD/file.json
    """
    date_pattern = r'\d{4}-\d{2}-\d{2}'
    match = re.search(date_pattern, file_path)
    if match:
        return match.group(0)
    raise ValueError(f"Could not extract date from {file_path}")

def process_file(file_path: str, schemas: Dict) -> Tuple[DynamicFrame, DynamicFrame]:
    """
    Process a single JSON file and convert to daily and intraday DynamicFrames
    """
    s3 = boto3.client('s3')
    
    try:
        response = s3.get_object(Bucket=bucket, Key=file_path)
        data = json.loads(response['Body'].read().decode('utf-8'))
        
        participant_id = extract_participant_id(file_path)
        daily_data, intraday_data = extract_activity_data(data, activity_type, participant_id)
        
        # Convert to DynamicFrames using the provided schemas
        daily_frame = DynamicFrame.fromDF(
            spark.createDataFrame(daily_data, schemas['daily']),
            glueContext,
            "daily_frame"
        )
        
        intraday_frame = DynamicFrame.fromDF(
            spark.createDataFrame(intraday_data, schemas['intraday']),
            glueContext,
            "intraday_frame"
        )
        
        return daily_frame, intraday_frame
        
    except Exception as e:
        print(f"Error processing file {file_path}: {str(e)}")
        raise

def process_heart_zones(summary: Dict[str, Any], participant_id: str, daily_data: List[Dict]) -> None:
    """
    Process heart rate zones data from daily summary
    """
    date = summary.get('dateTime')
    zones = summary.get('value', {}).get('heartRateZones', [])
    
    for zone in zones:
        daily_data.append({
            'participantidentifier': participant_id,
            'date': datetime.strptime(date, '%Y-%m-%d').date(),
            'zone_name': zone.get('name'),
            'calories_out': float(zone.get('caloriesOut', 0)),
            'min_heart_rate': int(zone.get('min', 0)),
            'max_heart_rate': int(zone.get('max', 0)),
            'minutes': float(zone.get('minutes', 0))
        })

def process_standard_daily(summary: Dict[str, Any], participant_id: str, activity_type: str, daily_data: List[Dict]) -> None:
    """
    Process standard daily summary data for non-heart-rate activities
    """
    daily_data.append({
        'participantidentifier': participant_id,
        'date': datetime.strptime(summary.get('dateTime'), '%Y-%m-%d').date(),
        'value': float(summary.get('value', 0))
    })

def process_intraday_data(dataset: List[Dict], participant_id: str, activity_type: str, intraday_data: List[Dict]) -> None:
    """
    Process intraday (minute-by-minute) activity data
    """
    for record in dataset:
        time_str = record.get('time')
        date_str = record.get('date', datetime.now().strftime('%Y-%m-%d'))
        
        # Combine date and time
        datetime_str = f"{date_str} {time_str}"
        timestamp = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')
        
        intraday_record = {
            'participantidentifier': participant_id,
            'datetime': timestamp,
            'value': float(record.get('value', 0))
        }
        
        # Add additional fields for calories if applicable
        if activity_type == 'calories':
            intraday_record.update({
                'level': int(record.get('level', 0)),
                'mets': float(record.get('mets', 0))
            })
            
        intraday_data.append(intraday_record)

def main():
    args = getResolvedOptions(sys.argv, [
        'JOB_NAME',
        'activity_type',
        'source_bucket',
        'source_prefix',
        'dest_bucket',
        'dest_prefix'
    ])
    
    sc = SparkContext()
    glueContext = GlueContext(sc)
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    process_activity_files(
        glueContext,
        args['activity_type'],
        args['source_bucket'],
        args['source_prefix']
    )
    
    job.commit()

if __name__ == "__main__":
    main()

================
File: glue_scripts/schemas.py
================
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.types import *


calories_daily_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("value", DoubleType(), True)
])

calories_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True),
    StructField("level", IntegerType(), True),
    StructField("mets", DoubleType(), True)
])

distance_daily_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("value", DoubleType(), True)
])

distance_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True)
])

elevation_daily_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("value", DoubleType(), True)
])

elevation_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True)
])

floors_daily_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("value", DoubleType(), True)
])

floors_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True)
])

heart_zones_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("zone_name", StringType(), True),
    StructField("calories_out", DoubleType(), True),
    StructField("min_heart_rate", IntegerType(), True),
    StructField("max_heart_rate", IntegerType(), True),
    StructField("minutes", DoubleType(), True)
])

heart_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True)
])

steps_daily_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("date", DateType(), True),
    StructField("value", DoubleType(), True)
])

steps_intraday_schema = StructType([
    StructField("participantidentifier", StringType(), True),
    StructField("datetime", TimestampType(), True),
    StructField("value", DoubleType(), True)
])

# Schema mapping for easy access
schema_mapping = {
    'calories': {
        'daily': calories_daily_schema,
        'intraday': calories_intraday_schema
    },
    'distance': {
        'daily': distance_daily_schema,
        'intraday': distance_intraday_schema
    },
    'elevation': {
        'daily': elevation_daily_schema,
        'intraday': elevation_intraday_schema
    },
    'floors': {
        'daily': floors_daily_schema,
        'intraday': floors_intraday_schema
    },
    'heart': {
        'daily': heart_zones_schema,
        'intraday': heart_intraday_schema
    },
    'steps': {
        'daily': steps_daily_schema,
        'intraday': steps_intraday_schema
    }
}

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.env
.venv
pip-log.txt
pip-delete-this-directory.txt

# IDE specific files
.idea/
.vscode/
*.swp
*.swo
.project
.pydevproject
.settings/

# Distribution / packaging
.Python
*.manifest
*.spec

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Jupyter Notebook
.ipynb_checkpoints

# Data and Logs
*.log

logs/
*.csv
*.dat
*.out

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db 
EDA.ipynb
check_folder_duplicates.py
debug_reprocessing.py
s3_size.py
schema_check.py
redshift_conn.py
data/parquet_files
data/unzipped_files
data/parquet_files_1
archive/
src/

================
File: README.md
================
# Fitbit Intraday Data Processing Pipeline

A cloud-based data processing pipeline that extracts Fitbit intraday activity data from S3, transforms it into Parquet format, and loads it into Amazon Redshift using AWS Glue and Apache Airflow.

## Architecture Overview

![Architecture Diagram](./docs/assets/architecture.png)

The pipeline consists of four main components:

1. **Data Processing**
   - Parallel processing of 6 activity types:
     - Heart Rate
     - Calories
     - Steps
     - Elevation
     - Distance
     - Floors
   - JSON to Parquet conversion
   - Staging and Production data layers

2. **Orchestration**
   - Apache Airflow for workflow management
   - Weekly scheduled execution
   - Task dependency management

3. **Data Flow**
   - Source: S3 (Fitbit JSON Data)
   - Processing: AWS Glue ETL
   - Destination: Amazon Redshift

4. **Error Handling**
   - Retry mechanisms
   - Data validation checks
   - Error notifications

## Data Processing Details

This pipeline handles the following types of Fitbit activity data:
- Heart rate and heart rate zones 
- Calories burned
- Steps taken
- Elevation
- Distance traveled
- Floors climbed

Data is processed at both daily summary and intraday (minute-by-minute) levels.

## Prerequisites

- AWS Account with access to:
  - S3 buckets (source and destination)
  - AWS Glue
  - Amazon Redshift
  - IAM roles and permissions
- Apache Airflow environment
- Python 3.6+

## Key Features

- Parallel activity processing
- In-memory metadata tracking
- Two-stage loading (staging to production)
- Automated error handling and retries
- Data validation and quality checks

## Configuration

### Environment Variables

The pipeline can be configured using environment variables:

#### S3 Configuration
- `FITBIT_SOURCE_BUCKET`: Source bucket for Fitbit data
- `FITBIT_SOURCE_PREFIX`: Prefix for Fitbit data files
- `FITBIT_DEST_BUCKET`: Destination bucket for processed data
- `FITBIT_DEST_PREFIX`: Prefix for processed data
- `GLUE_SCRIPTS_BUCKET`: Bucket containing Glue scripts

#### Redshift Configuration
- `REDSHIFT_DATABASE`: Redshift database name
- `REDSHIFT_SCHEMA`: Redshift schema name (default: 'rural')
- `REDSHIFT_TEMP_DIR`: S3 location for temporary files

#### AWS Configuration
- `AWS_REGION`: AWS region for services
- `GLUE_JOB_PREFIX`: Prefix for Glue job names

### Airflow Configuration

1. Set up Redshift connection in Airflow:
   ```bash
   airflow connections add 'redshift_default' \
     --conn-type 'postgres' \
     --conn-host 'your-redshift-host' \
     --conn-schema 'your-database' \
     --conn-login 'your-username' \
     --conn-password 'your-password' \
     --conn-port 5439
   ```

2. Set up AWS connection:
   ```bash
   airflow connections add 'aws_default' \
     --conn-type 'aws' \
     --conn-login 'your-access-key-id' \
     --conn-password 'your-secret-access-key' \
     --conn-extra '{"region_name": "your-region"}'
   ```

### Local Development

For local development, create a `.env` file:

```env
# S3 Configuration
FITBIT_SOURCE_BUCKET=umms-research-radx-image-processing
FITBIT_SOURCE_PREFIX=Onboarding/RADx/RURAL/fitbit_intraday
FITBIT_DEST_BUCKET=umasschan-plum-data-lab
FITBIT_DEST_PREFIX=Rural/intraday
GLUE_SCRIPTS_BUCKET=your-glue-scripts-bucket

# Redshift Configuration
REDSHIFT_DATABASE=your_database
REDSHIFT_SCHEMA=rural
REDSHIFT_TEMP_DIR=s3://your-temp-bucket/temp/

# AWS Configuration
AWS_REGION=us-east-1
GLUE_JOB_PREFIX=rural_fitbit
```

### Production Deployment

For production deployment:

1. Set environment variables in your Airflow environment
2. Configure AWS Glue job parameters in the AWS console
3. Ensure proper IAM roles and permissions are set up

## Error Handling

The pipeline includes comprehensive error handling:
- Automatic retries for failed tasks
- Data validation checks
- Error notifications
- Processing statistics tracking

## Monitoring

Monitor the pipeline through:
- Airflow web interface
- AWS CloudWatch logs
- Processing statistics
- Error notifications

## Contributing

Please refer to CONTRIBUTING.md for guidelines on contributing to this project.

## License

[Your License Information]



================================================================
End of Codebase
================================================================
